{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db822fa-f424-49c4-be81-d44350350508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "data_path = 'data'\n",
    "\n",
    "\n",
    "path = Path(data_path)\n",
    "\n",
    "if not path.exists():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "with zipfile.ZipFile('train.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "249763e9-2987-43a6-889a-e0ce342c2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class TestImagesDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = [f for f in os.listdir(root_dir)\n",
    "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = TestImagesDataset(\"data/test\", transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a811dc9e-9749-4eab-bd78-4bf03e0e1f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.1320, acc=0.6695\n",
      "Val:   loss=0.6632, acc=0.9624\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.9771, acc=0.7412\n",
      "Val:   loss=0.6498, acc=0.9667\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.9871, acc=0.6965\n",
      "Val:   loss=0.6207, acc=0.9733\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.9469, acc=0.7044\n",
      "Val:   loss=0.5872, acc=0.9768\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.9769, acc=0.7005\n",
      "Val:   loss=0.6003, acc=0.9740\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = 'data'\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "test_dir = os.path.join(data_path, \"test\")\n",
    "\n",
    "class_names = ['cat','elephant','butterfly','sheep','spider',\n",
    "               'horse','dog','cow','chicken','squirrel']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "image_size = (224, 224)\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha <= 0:\n",
    "        return x, y, 1\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"Возвращает смешанные входы и цели (CutMix).\"\"\"\n",
    "    if alpha <= 0:\n",
    "        return x, y, 1\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size, _, h, w = x.size()\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    cut_w = int(w * np.sqrt(1 - lam))\n",
    "    cut_h = int(h * np.sqrt(1 - lam))\n",
    "    cx = np.random.randint(w)\n",
    "    cy = np.random.randint(h)\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, w)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, w)\n",
    "    y1 = np.clip(cy - cut_h // 2, 0, h)\n",
    "    y2 = np.clip(cy + cut_h // 2, 0, h)\n",
    "    x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]\n",
    "    lam = 1 - ((x2 - x1) * (y2 - y1) / (w * h))\n",
    "    y_a, y_b = y, y[index]\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.5):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * (image_size[0] // 8) * (image_size[1] // 8), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "def build_resnet(num_classes=10, fine_tune_at=-50, dropout_rate=0.5):\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    if fine_tune_at is not None:\n",
    "        for param in list(model.parameters())[fine_tune_at:]:\n",
    "            param.requires_grad = True\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, use_mixup=True, use_cutmix=False):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for imgs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Применяем MixUp или CutMix\n",
    "        if use_mixup and random.random() < 0.5:\n",
    "            imgs, y_a, y_b, lam = mixup_data(imgs, labels)\n",
    "            outputs = model(imgs)\n",
    "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "        elif use_cutmix and random.random() < 0.5:\n",
    "            imgs, y_a, y_b, lam = cutmix_data(imgs, labels)\n",
    "            outputs = model(imgs)\n",
    "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "        else:\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "use_resnet = True    \n",
    "optimizer_name = \"adam\"  \n",
    "\n",
    "model = build_resnet(num_classes, fine_tune_at=-50) if use_resnet else CustomCNN(num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "if optimizer_name == \"adam\":\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "elif optimizer_name == \"rmsprop\":\n",
    "    optimizer = optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "else:\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, momentum=0.9)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "early_stop_patience = 3\n",
    "best_val_acc = 0.0\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, use_mixup=True, use_cutmix=True)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.4f}\")\n",
    "    print(f\"Val:   loss={val_loss:.4f}, acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= early_stop_patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1121ccc3-3321-4dfb-80bc-d76e77448078",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbe2bf0e-b7e5-496b-b10a-9f61c0ffb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "ret = {'butterfly': 2, \n",
    "       'cat': 0, \n",
    "       'chicken': 8, \n",
    "       'cow': 7, \n",
    "       'dog': 6,\n",
    "       'elephant': 1,\n",
    "       'horse': 5,\n",
    "       'sheep': 3,\n",
    "       'spider': 4,\n",
    "       'squirrel': 9}\n",
    "\n",
    "class_names = list(ret.keys())\n",
    "\n",
    "model.eval()\n",
    "ids = []\n",
    "labels =  []\n",
    "with torch.no_grad():\n",
    "    for images, filenames in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        for fname, pred in zip(filenames, preds.cpu().numpy()):\n",
    "            ids.append(fname)\n",
    "            labels.append(ret[class_names[pred]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a337368-b63c-498f-a613-d595e447f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1593feeb-4ce6-4d74-b5c1-cb200e8b4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6846e33-13c2-4430-bf13-f1b77244e991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0, 'elephant': 1, 'butterfly': 2, 'sheep': 3, 'spider': 4, 'horse': 5, 'dog': 6, 'cow': 7, 'chicken': 8, 'squirrel': 9}\n",
      "['cat', 'elephant', 'butterfly', 'sheep', 'spider', 'horse', 'dog', 'cow', 'chicken', 'squirrel']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.class_to_idx)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "891a9c90-5536-4673-8065-5a31ad7cdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(train_dataset.class_to_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0137e729-ed08-4462-aa2e-593abfac9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'butterfly': 0, 'cat': 1, 'chicken': 2, 'cow': 3, 'dog': 4, 'elephant': 5, 'horse': 6, 'sheep': 7, 'spider': 8, 'squirrel': 9}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.class_to_idx)\n",
    "#print(test_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "257b8681-1e46-4cd9-8388-a256d83d9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.ImageFolder(test_dir, transform=val_test_transforms)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab9cc8b-e1ad-459c-8d1f-d4dd242aace3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.ipynb_checkpoints': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3da9c739-2dd0-4872-b34a-38f75c070435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 1\n",
       "    Root location: data/test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               CenterCrop(size=(224, 224))\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3489ea84-35a7-4e6d-af81-790d67647d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 08:59:08.165585: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-18 08:59:08.192927: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/user/ML/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "2025-10-18 08:59:08.757328: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21949 files belonging to 10 classes.\n",
      "Using 17560 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1760777949.595580  302420 gpu_device.cc:2431] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1760777949.605847  302420 gpu_device.cc:2431] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "I0000 00:00:1760777949.619854  302420 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 27191 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:01:00.0, compute capability: 12.0\n",
      "2025-10-18 08:59:09.830705: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX'\n",
      "\n",
      "2025-10-18 08:59:09.830720: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\n",
      "2025-10-18 08:59:09.830725: W tensorflow/core/framework/op_kernel.cc:1842] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "2025-10-18 08:59:09.830730: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "{{function_node __wrapped__Equal_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Equal] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     29\u001b[39m BATCH_SIZE = \u001b[32m64\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Создаём датасеты (train/val/test)\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Если у вас нет отдельного test — можно разделить train на train+val как раньше.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m train_dataset = \u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     43\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m val_dataset = image_dataset_from_directory(\n\u001b[32m     46\u001b[39m     train_dir,\n\u001b[32m     47\u001b[39m     image_size=IMAGE_SIZE,\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     seed=\u001b[32m42\u001b[39m\n\u001b[32m     53\u001b[39m )\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Опционально: если есть тестовая папка\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/.venv/lib/python3.12/site-packages/keras/src/utils/image_dataset_utils.py:334\u001b[39m, in \u001b[36mimage_dataset_from_directory\u001b[39m\u001b[34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_paths:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    330\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo images found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    331\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAllowed formats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWLIST_FORMATS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m dataset = \u001b[43mpaths_and_labels_to_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcrop_to_aspect_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrop_to_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_aspect_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle_buffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    351\u001b[39m     dataset = dataset.batch(batch_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/.venv/lib/python3.12/site-packages/keras/src/utils/image_dataset_utils.py:389\u001b[39m, in \u001b[36mpaths_and_labels_to_dataset\u001b[39m\u001b[34m(image_paths, image_size, num_channels, labels, label_mode, num_classes, interpolation, data_format, crop_to_aspect_ratio, pad_to_aspect_ratio, shuffle, shuffle_buffer_size, seed)\u001b[39m\n\u001b[32m    386\u001b[39m     ds = path_ds\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     ds = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle_buffer_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m args = (\n\u001b[32m    392\u001b[39m     image_size,\n\u001b[32m    393\u001b[39m     num_channels,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     pad_to_aspect_ratio,\n\u001b[32m    398\u001b[39m )\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label_mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:1510\u001b[39m, in \u001b[36mDatasetV2.shuffle\u001b[39m\u001b[34m(self, buffer_size, seed, reshuffle_each_iteration, name)\u001b[39m\n\u001b[32m   1419\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshuffle\u001b[39m(\n\u001b[32m   1420\u001b[39m     \u001b[38;5;28mself\u001b[39m, buffer_size, seed=\u001b[38;5;28;01mNone\u001b[39;00m, reshuffle_each_iteration=\u001b[38;5;28;01mTrue\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1421\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mDatasetV2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1422\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Randomly shuffles the elements of this dataset.\u001b[39;00m\n\u001b[32m   1423\u001b[39m \n\u001b[32m   1424\u001b[39m \u001b[33;03m  This dataset fills a buffer with `buffer_size` elements, then randomly\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1508\u001b[39m \u001b[33;03m    A new `Dataset` with the transformation applied as described above.\u001b[39;00m\n\u001b[32m   1509\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1510\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshuffle_op\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_shuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m   1511\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreshuffle_each_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/shuffle_op.py:32\u001b[39m, in \u001b[36m_shuffle\u001b[39m\u001b[34m(input_dataset, buffer_size, seed, reshuffle_each_iteration, name)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_shuffle\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[32m     26\u001b[39m     input_dataset,\n\u001b[32m     27\u001b[39m     buffer_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     name=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     31\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ShuffleDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreshuffle_each_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/shuffle_op.py:51\u001b[39m, in \u001b[36m_ShuffleDataset.__init__\u001b[39m\u001b[34m(self, input_dataset, buffer_size, seed, reshuffle_each_iteration, name)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m._input_dataset = input_dataset\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m._buffer_size = ops.convert_to_tensor(\n\u001b[32m     50\u001b[39m     buffer_size, dtype=dtypes.int64, name=\u001b[33m\"\u001b[39m\u001b[33mbuffer_size\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28mself\u001b[39m._seed, \u001b[38;5;28mself\u001b[39m._seed2 = \u001b[43mrandom_seed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mself\u001b[39m._reshuffle_each_iteration = reshuffle_each_iteration\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m._name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/.venv/lib/python3.12/site-packages/tensorflow/python/data/util/random_seed.py:50\u001b[39m, in \u001b[36mget_seed\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m     46\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(\u001b[33m\"\u001b[39m\u001b[33mseed2\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[32m     47\u001b[39m     seed2 = ops.convert_to_tensor(seed2, dtype=dtypes.int64)\n\u001b[32m     48\u001b[39m     seed2 = array_ops.where_v2(\n\u001b[32m     49\u001b[39m         math_ops.logical_and(\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m             \u001b[43mmath_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m, math_ops.equal(seed2, \u001b[32m0\u001b[39m)),\n\u001b[32m     51\u001b[39m         constant_op.constant(\u001b[32m2\u001b[39m**\u001b[32m31\u001b[39m - \u001b[32m1\u001b[39m, dtype=dtypes.int64),\n\u001b[32m     52\u001b[39m         seed2,\n\u001b[32m     53\u001b[39m         name=scope)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m seed, seed2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/.venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6027\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   6025\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   6026\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m6027\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mInternalError\u001b[39m: {{function_node __wrapped__Equal_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Equal] name: "
     ]
    }
   ],
   "source": [
    "# Требуется:\n",
    "# pip install -U tensorflow keras-tuner\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Rescaling, Input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import keras_tuner as kt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# Параметры общего пайплайна\n",
    "# ----------------------------\n",
    "data_path = 'data'  # ваша папка с train/val/test\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "test_dir = os.path.join(data_path, \"test\")  # убедитесь, что есть тестовый набор\n",
    "class_names = ['cat','elephant','butterfly','sheep','spider','horse','dog','cow','chicken','squirrel']\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Для ResNet — стандартный размер 224x224. Для кастомной сети можно уменьшать.\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# ----------------------------\n",
    "# Создаём датасеты (train/val/test)\n",
    "# ----------------------------\n",
    "# Если у вас нет отдельного test — можно разделить train на train+val как раньше.\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    class_names=class_names,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    class_names=class_names,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Опционально: если есть тестовая папка\n",
    "if os.path.exists(test_dir):\n",
    "    test_dataset = image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        class_names=class_names\n",
    "    )\n",
    "else:\n",
    "    test_dataset = None\n",
    "\n",
    "# Кеш/префетч\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "if test_dataset is not None:\n",
    "    test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# ----------------------------\n",
    "# Data Augmentation слой (общий)\n",
    "# ----------------------------\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.08),\n",
    "    layers.RandomZoom(0.08),\n",
    "    layers.RandomTranslation(0.05, 0.05),\n",
    "    layers.RandomContrast(0.08),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "# ----------------------------\n",
    "# Вариант A: Transfer learning с ResNet50V2\n",
    "#   - сначала тренируем только головой (feature extractor frozen)\n",
    "#   - затем \"размораживаем\" последние N слоёв и дообучаем с малым lr\n",
    "# ----------------------------\n",
    "def build_resnet_model(num_dense_units=512, dropout_rate=0.5, fine_tune_at=None):\n",
    "    # Вход\n",
    "    inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    # ResNet ожидает предобработку\n",
    "    x = layers.Lambda(resnet_preprocess)(x)\n",
    "    # Базовая модель\n",
    "    base_model = ResNet50V2(weights='imagenet', include_top=False, input_tensor=x)\n",
    "    base_model.trainable = False  # сначала заморожена\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(num_dense_units, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    # можно добавить ещё один плотный слой по желанию\n",
    "    outputs = Dense(len(class_names), activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Компиляция\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Если указали fine_tune_at (номер слоя), размораживаем tail\n",
    "    if fine_tune_at is not None:\n",
    "        base_model.trainable = True\n",
    "        # замораживаем все до fine_tune_at\n",
    "        for layer in base_model.layers[:fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "        # уменьшаем lr для тонкой настройки\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "# Коллбэки\n",
    "checkpoint_path = \"best_resnet_model.h5\"\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-7),\n",
    "    ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "]\n",
    "\n",
    "# Тренировка: сначала \"head\" несколько эпох, затем тонкая настройка\n",
    "resnet_model, base = build_resnet_model(num_dense_units=512, dropout_rate=0.5, fine_tune_at=None)\n",
    "initial_epochs = 5\n",
    "fine_tune_epochs = 5  # можно также 5, чтобы обе стадии были короткими\n",
    "\n",
    "history_head = resnet_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=initial_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Тонкая настройка (fine-tuning)\n",
    "fine_tune_at = -30  # можно варьировать: -50, -30, None\n",
    "if isinstance(fine_tune_at, int) and fine_tune_at < 0:\n",
    "    ft_at = len(base.layers) + fine_tune_at\n",
    "else:\n",
    "    ft_at = fine_tune_at\n",
    "\n",
    "resnet_model, base = build_resnet_model(num_dense_units=512, dropout_rate=0.5, fine_tune_at=ft_at)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    try:\n",
    "        resnet_model.load_weights(checkpoint_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "history_finetune = resnet_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=fine_tune_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "if test_dataset is not None:\n",
    "    test_loss, test_acc = resnet_model.evaluate(test_dataset)\n",
    "    print(f\"ResNet test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "def build_custom_cnn(hp):\n",
    "    inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    x = layers.Rescaling(1./255)(x)\n",
    "    \n",
    "    num_blocks = hp.Int(\"num_blocks\", 1, 4, default=2)           \n",
    "    filters0 = hp.Int(\"filters0\", 32, 128, step=32, default=32)\n",
    "    convs_per_block = hp.Int(\"convs_per_block\", 1, 3, default=2) \n",
    "    kernel_size = hp.Choice(\"kernel_size\", [3,5,7], default=3)  \n",
    "    dropout_rate = hp.Float(\"dropout_rate\", 0.2, 0.6, step=0.1, default=0.4)\n",
    "    use_extra_dense = hp.Boolean(\"use_extra_dense\", default=True)\n",
    "    dense_units = hp.Int(\"dense_units\", 128, 1024, step=128, default=512)\n",
    "    num_dense_layers = hp.Int(\"num_dense_layers\", 1, 2, default=1)\n",
    "\n",
    "    x_filters = filters0\n",
    "    for b in range(num_blocks):\n",
    "        for c in range(convs_per_block):\n",
    "            x = Conv2D(x_filters, (kernel_size, kernel_size), padding='same', activation='relu')(x)\n",
    "        x = MaxPooling2D((2,2))(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x_filters *= 2\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    for d in range(num_dense_layers):\n",
    "        x = Dense(dense_units, activation='relu')(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(len(class_names), activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    opt_choice = hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"], default=\"adam\")\n",
    "    if opt_choice == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "    elif opt_choice == \"rmsprop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop()\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Keras Tuner setup\n",
    "tuner_dir = \"kt_tuner\"\n",
    "tuner = kt.RandomSearch(\n",
    "    build_custom_cnn,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,       # увеличьте при наличии вычислительных ресурсов\n",
    "    executions_per_trial=1,\n",
    "    directory=tuner_dir,\n",
    "    project_name=\"cnn_search\"\n",
    ")\n",
    "\n",
    "# Вывод краткой сводки перед запуском\n",
    "print(\"Начинаем поиск архитектур кастомной сети (Keras Tuner). Это может занять долгое время.\")\n",
    "\n",
    "# Run search (можно контролировать epochs и batch_size)\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    epochs=15,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")\n",
    "\n",
    "# Лучшая модель\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "# Оценка на тесте\n",
    "if test_dataset is not None:\n",
    "    loss, acc = best_model.evaluate(test_dataset)\n",
    "    print(f\"Best custom CNN test accuracy: {acc:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Полезные замечания по поиску гиперпараметров и предотвращению переобучения\n",
    "# ----------------------------\n",
    "# 1) Тщательно следите за графиками train/val loss и accuracy — если train >> val (val хуже), значит переобучение.\n",
    "# 2) Увеличивайте Data Augmentation и/или собирайте больше данных при выраженном переобучении.\n",
    "# 3) Используйте EarlyStopping и ReduceLROnPlateau — в коде включено.\n",
    "# 4) Попробуйте разные значения fine_tune_at (размораживать побольше/меньше слоёв ResNet).\n",
    "# 5) Экспериментируйте с image size: 224 или 299 дают больше информации, но медленнее.\n",
    "# 6) Попробуйте балансировать классы (class_weight) если у вас несбалансированные классы.\n",
    "# 7) Попробуйте MixUp / CutMix augmentation для лучшей обобщающей способности.\n",
    "# 8) Используйте test_dataset только для финальной оценки. Для подбора гиперпараметров используйте val_dataset.\n",
    "\n",
    "# Пример генерации class_weight при дисбалансе:\n",
    "# from sklearn.utils import class_weight\n",
    "# y = []  # собрать метки из train_dataset\n",
    "# for batch_x, batch_y in train_dataset.unbatch().batch(10000): y = np.concatenate([y, batch_y.numpy()])\n",
    "# class_weights = class_weight.compute_class_weight(\"balanced\", classes=np.unique(y), y=y)\n",
    "# class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Затем передавайте class_weight=class_weights_dict в model.fit(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b1ab9ff-ab4a-4884-9ef0-976f43943abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n",
      "12.5.1\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
    "print(tf.sysconfig.get_build_info()[\"cudnn_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0edbc00-b9be-4d11-b884-0bc158eb21eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
